{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "import yaml\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from langchain import hub\n",
    "from openai import OpenAI\n",
    "from scipy.spatial.distance import cosine\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from typing_extensions import Literal\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv('.env')\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get('OPENAI_API_KEY')\n",
    "key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing: Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the LangChain loader to load a narrative text\n",
    "loader = TextLoader(\"try-to-remember.txt\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500, chunk_overlap=500, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 80 splits\n",
      "\n",
      "First split:\n",
      "\n",
      "Every mind on earth capable of understanding the problem was focused on the spaceship and the ultimatum delivered by its occupants. Talk or Die! blared the newspaper headlines.\n",
      "\n",
      "The suicide rate was up and still climbing. Religious cults were having a field day. A book by a science fiction author: \"What the Deadly Inter-Galactic Spaceship Means to You!\" had smashed all previous best-seller records. And this had been going on for a frantic seven months.\n",
      "\n",
      "The ship had flapped out of a gun-metal sky over Oregon, its shape that of a hideously magnified paramecium with edges that rippled like a mythological flying carpet. Its five green-skinned, froglike occupants had delivered the ultimatum, one copy printed on velvety paper to each major government, each copy couched faultlessly in the appropriate native tongue:\n",
      "\n",
      "\"You are requested to assemble your most gifted experts in human communication. We are about to submit a problem. We will open five identical rooms of our vessel to you. One of us will be available in each room.\n",
      "\n",
      "\"Your problem: To communicate with us.\n",
      "\n",
      "\"If you succeed, your rewards will be great.\n",
      "\n",
      "\"If you fail, that will result in destruction for all sentient life on your planet.\n",
      "\n",
      "\"We announce this threat with the deepest regret. You are urged to examine Eniwetok atoll for a small display of our power. Your artificial satellites have been removed from the skies.\n",
      "\n",
      "\"You must break away from this limited communication!\"\n"
     ]
    }
   ],
   "source": [
    "# Print the number of splits\n",
    "print(f\"There are {len(all_splits)} splits\\n\")\n",
    "\n",
    "# Print the first split\n",
    "print(\"First split:\\n\")\n",
    "print(f\"{all_splits[0].page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing: Store the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the document chunks in a vector database using the Chroma module from the LangChain library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of the narrative text. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 6\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": number_of_chunks})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"Who is Ohashi?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She thought about Hiko Ohashi: a strange man. He was fifty and didn't look a day over thirty. He had grown children. His wife had died of cholera eight years ago. Francine wondered what it would be like married to an Oriental, and she found herself thinking that he wasn't really Oriental with his Princeton education and Occidental ways. Then she realized that this attitude was a kind of white snobbery.\n",
      "\n",
      "The door in the corner of the room opened softly. Ohashi came in, closed the door. \"You awake?\" he whispered.\n",
      "\n",
      "She turned her head without lifting it from the chairback. \"Yes.\"\n",
      "\n",
      "\"I'd hoped you might fall asleep for a bit,\" he said. \"You looked so tired when I left.\"\n",
      "\n",
      "Francine glanced at her wristwatch. \"It's only three-thirty. What's the day like?\"\n",
      "\n",
      "\"Hot and windy.\"\n",
      "\n",
      "Ohashi busied himself inserting film into the projector at the rear of the room. Presently, he went to his chair, trailing the remote control cable for the projector.\n",
      "\n",
      "\"Ready?\" he asked.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "example_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: filler question \n",
      "Context: filler context \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Galactics threatened to destroy humanity if it failed to learn to communicate unmistakably, implying the complete destruction of the human race."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"Who did the Galactics threaten to destroy if humanity failed?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the pipeline to the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the Q&A dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('literature-qa-dataset.yml', 'r') as file:\n",
    "    dataset = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a schema for the answers, using the Pydantic `BaseModel` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer(BaseModel):\n",
    "    \"\"\"The answer to a multiple-choice question\"\"\"\n",
    "\n",
    "    answer: Literal[\"A\", \"B\", \"C\", \"D\"] = Field(description=\"The answer to the question\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's define a couple of helper functions to reuse between the different pipelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_answers(wrong_answers, correct_answer):\n",
    "    # Shuffle the answers\n",
    "    answers = wrong_answers + [correct_answer]\n",
    "    random.shuffle(answers)\n",
    "    \n",
    "    # Identify the correct answer\n",
    "    correct_index = answers.index(q['correct_answer'])\n",
    "    correct_letter = ['A', 'B', 'C', 'D'][correct_index]\n",
    "\n",
    "    return answers, correct_letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to  create a [ChatPromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html).\n",
    "\n",
    "The template contains a system message which instructs the model to extract the right answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_baseline_llm = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm.\"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        (\"human\", \"\"\"Answer the following question:\n",
    "         \n",
    "        {text}\"\"\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we iterate over the dataset and apply the pipeline to each question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What threat did the newspaper headlines blare?\n",
      "Answer: A (Listen or Die!)\n",
      "The correct answer: B (Talk or Die!)\n",
      "\n",
      "The answer was correct: False\n",
      "\n",
      "Question: What is the name of the protagonist?\n",
      "Answer: C (Theodore Zakheim)\n",
      "The correct answer: D (Francine Millar)\n",
      "\n",
      "The answer was correct: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in dataset['questions']:\n",
    "    # Create the runnable using the prompt, LLM and schema for the answer\n",
    "    runnable = prompt_baseline_llm | llm.with_structured_output(schema=Answer)\n",
    "    \n",
    "    # Shuffle the answers and get the letter of the correct answer\n",
    "    answers, correct_letter = shuffle_answers(q['wrong_answers'], q['correct_answer'])\n",
    "    \n",
    "    # Create the text with question and answers\n",
    "    text = f\"\"\"{q['question']}\n",
    "    A) {answers[0]}\n",
    "    B) {answers[1]}\n",
    "    C) {answers[2]}\n",
    "    D) {answers[3]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Invoke the runnable\n",
    "    response = runnable.invoke(\n",
    "        {\n",
    "            \"text\": text\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Print the results\n",
    "    answer_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "    print(f\"Question: {q['question']}\")\n",
    "    print(f\"Answer: {response.answer} ({answers[answer_mapping[response.answer]]})\")\n",
    "    print(f\"The correct answer: {correct_letter} ({q['correct_answer']})\\n\")\n",
    "    print(f\"The answer was correct: {response.answer == correct_letter}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline LLM with full book as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"try-to-remember.txt\", \"r\") as file:\n",
    "    full_book = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rag = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an expert extraction algorithm.\"\n",
    "            \"Only extract relevant information from the text. \"\n",
    "            \"If you do not know the value of an attribute asked to extract, \"\n",
    "            \"return null for the attribute's value.\",\n",
    "        ),\n",
    "        (\"human\", \"\"\"Answer the following question:\n",
    "         \n",
    "        {text}\n",
    "         \n",
    "        Based your answer on the following narrative text:\n",
    "        \n",
    "        {context}\"\"\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What threat did the newspaper headlines blare?\n",
      "Answer: A (Talk or Die!)\n",
      "The correct answer: A (Talk or Die!)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What is the name of the protagonist?\n",
      "Answer: D (Francine Millar)\n",
      "The correct answer: D (Francine Millar)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: In which US state did the spaceship initially descend?\n",
      "Answer: A (Oregon)\n",
      "The correct answer: A (Oregon)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: How many extraterrestrial beings were on the spaceship?\n",
      "Answer: A (Five)\n",
      "The correct answer: A (Five)\n",
      "\n",
      "The answer was correct: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in dataset['questions']:\n",
    "    runnable = prompt_rag | llm.with_structured_output(schema=Answer)\n",
    "    \n",
    "    # Shuffle the answers and get the letter of the correct answer\n",
    "    answers, correct_letter = shuffle_answers(q['wrong_answers'], q['correct_answer'])\n",
    "    \n",
    "    # Create the text with question and answers\n",
    "    text = f\"\"\"{q['question']}\n",
    "\n",
    "    Answer with only the letter of the correct answer:\n",
    "    A) {answers[0]}\n",
    "    B) {answers[1]}\n",
    "    C) {answers[2]}\n",
    "    D) {answers[3]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Invoke the runnable\n",
    "    response = runnable.invoke(\n",
    "        {\n",
    "        \"text\": text,\n",
    "        \"context\": full_book\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    answer_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "    print(f\"Question: {q['question']}\")\n",
    "    print(f\"Answer: {response.answer} ({answers[answer_mapping[response.answer]]})\")\n",
    "    print(f\"The correct answer: {correct_letter} ({q['correct_answer']})\\n\")\n",
    "    print(f\"The answer was correct: {response.answer == correct_letter}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What threat did the newspaper headlines blare?\n",
      "Answer: D (Talk or Die!)\n",
      "The correct answer: D (Talk or Die!)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What is the name of the protagonist?\n",
      "Answer: C (Francine Millar)\n",
      "The correct answer: C (Francine Millar)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: In which US state did the spaceship initially descend?\n",
      "Answer: A (Oregon)\n",
      "The correct answer: A (Oregon)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: How many extraterrestrial beings were on the spaceship?\n",
      "Answer: B (Five)\n",
      "The correct answer: B (Five)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: Which site was used by the aliens to demonstrate their destructive capabilities?\n",
      "Answer: C (Eniwetok atoll)\n",
      "The correct answer: C (Eniwetok atoll)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What field does Francine Millar specialize in?\n",
      "Answer: C (Clinical psychology)\n",
      "The correct answer: C (Clinical psychology)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What did Francine have with her when she walked through the sand?\n",
      "Answer: A (Water bottle)\n",
      "The correct answer: C (Briefcase)\n",
      "\n",
      "The answer was correct: False\n",
      "\n",
      "Question: What was the cause of Francine's husband's death?\n",
      "Answer: A (Plane crash)\n",
      "The correct answer: A (Plane crash)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: Who is Francine's colleague from the Japanese-Korean and Sino-Tibetan research team?\n",
      "Answer: D (Hikonojo Ohashi)\n",
      "The correct answer: D (Hikonojo Ohashi)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: Which satellites were destroyed by the aliens?\n",
      "Answer: A (Russian and United States satellites)\n",
      "The correct answer: A (Russian and United States satellites)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What were humans challenged to do by the aliens?\n",
      "Answer: D (To interact with them)\n",
      "The correct answer: D (To interact with them)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: Who is the other Dr. Millar mentioned in the story?\n",
      "Answer: C (Francine's late spouse)\n",
      "The correct answer: C (Francine's late spouse)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: How is Francine described in terms of her physical appearance?\n",
      "Answer: C (Wiry and slim, auburn hair, blue eyes, sleepy look)\n",
      "The correct answer: C (Wiry and slim, auburn hair, blue eyes, sleepy look)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What was the tone of the alien's threat?\n",
      "Answer: B (Defiant)\n",
      "The correct answer: C (Regretful)\n",
      "\n",
      "The answer was correct: False\n",
      "\n",
      "Question: What revealed Emile Goré's accompaniment to Francine?\n",
      "Answer: A (His light tread)\n",
      "The correct answer: A (His light tread)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What do the Hottentot-Bushmen, the Ainu, the Basque and the Australian-Papuan languages have in common, according to the story?\n",
      "Answer: D (They are not represented among the experts.)\n",
      "The correct answer: D (They are not represented among the experts.)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: Where is the Hermiston Ordnance Depot located relative to the spaceship's landing site?\n",
      "Answer: D (South)\n",
      "The correct answer: B (North)\n",
      "\n",
      "The answer was correct: False\n",
      "\n",
      "Question: Who did the Galactics threaten to destroy if humanity failed?\n",
      "Answer: B (Every conscious being)\n",
      "The correct answer: B (Every conscious being)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: Which character would know most about Mongolian?\n",
      "Answer: B (Theodore Zakheim)\n",
      "The correct answer: B (Theodore Zakheim)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: How did the Javanese monk see the aliens?\n",
      "Answer: B (Highly spiritual beings)\n",
      "The correct answer: B (Highly spiritual beings)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What motivates the Earth's experts to cooperate with each other?\n",
      "Answer: C (The threat of annihilation)\n",
      "The correct answer: C (The threat of annihilation)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What does the Galactic's response to the military's attack suggest about their intentions?\n",
      "Answer: D (They are preparing for retaliation.)\n",
      "The correct answer: B (They are not interested in war.)\n",
      "\n",
      "The answer was correct: False\n",
      "\n",
      "Question: Why were Francine and Ohashi better than others at understanding the aliens?\n",
      "Answer: A (They were more open-minded.)\n",
      "The correct answer: A (They were more open-minded.)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What is implied about humanity's future after Francine's realization?\n",
      "Answer: A (It will be shaped by cooperation.)\n",
      "The correct answer: A (It will be shaped by cooperation.)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: Why do the Galactics issue an ultimatum to humanity?\n",
      "Answer: A (To force effective communication)\n",
      "The correct answer: A (To force effective communication)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: Why is Francine particularly motivated to solve the communication problem?\n",
      "Answer: C (The death of her husband and its connection to the aliens.)\n",
      "The correct answer: C (The death of her husband and its connection to the aliens.)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What do the aliens' physical traits suggest about their origin?\n",
      "Answer: B (They evolved in a low-gravity environment.)\n",
      "The correct answer: C (They evolved in an aquatic environment.)\n",
      "\n",
      "The answer was correct: False\n",
      "\n",
      "Question: What does the alien's choice of demonstration site reveal about their intentions?\n",
      "Answer: A (They seek to avoid killing humans.)\n",
      "The correct answer: A (They seek to avoid killing humans.)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: Why do the military leader lie to Francine about the nature of the aliens?\n",
      "Answer: C (To manipulate her into aiding their plans.)\n",
      "The correct answer: C (To manipulate her into aiding their plans.)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What does Francine's outburst at the aliens reveal to them?\n",
      "Answer: C (Humans' capacity for emotion and honest communication.)\n",
      "The correct answer: C (Humans' capacity for emotion and honest communication.)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: How is emotion depicted in the story?\n",
      "Answer: B (A crucial element in communication and understanding.)\n",
      "The correct answer: B (A crucial element in communication and understanding.)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What does Francine's journey represent in the story?\n",
      "Answer: A (Overcoming personal loss and gaining emotional insight.)\n",
      "The correct answer: A (Overcoming personal loss and gaining emotional insight.)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What is the paradox of language in the story?\n",
      "Answer: A (It's both a barrier and a bridge to understanding.)\n",
      "The correct answer: A (It's both a barrier and a bridge to understanding.)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: What does the story suggest about cultural diversity?\n",
      "Answer: D (It enriches communication and deepens understanding.)\n",
      "The correct answer: D (It enriches communication and deepens understanding.)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Question: How is biblical allegory used in the story?\n",
      "Answer: B (As a myth of forbidden knowledge and divine retribution.)\n",
      "The correct answer: D (As a cautionary tale about the consequences of misunderstanding.)\n",
      "\n",
      "The answer was correct: False\n",
      "\n",
      "Question: What is the significance of Francine shaking her fists at the aliens?\n",
      "Answer: C (It shows her anger and frustration in misunderstanding.)\n",
      "The correct answer: A (It shows her honesty and passion in communication.)\n",
      "\n",
      "The answer was correct: False\n",
      "\n",
      "Question: What role does trust play in the story?\n",
      "Answer: C (It leads to uncertainty and betrayal.)\n",
      "The correct answer: A (It's crucial for cooperation and understanding.)\n",
      "\n",
      "The answer was correct: False\n",
      "\n",
      "Question: What does the story imply about the consequences of miscommunication?\n",
      "Answer: C (It can be catastropic and lead to planetary destruction.)\n",
      "The correct answer: C (It can be catastropic and lead to planetary destruction.)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Correct answers: 30\n",
      "Total questions: 38\n",
      "Accuracy: 0.7894736842105263\n",
      "\n"
     ]
    }
   ],
   "source": [
    "count_correct = 0\n",
    "for q in dataset['questions']:\n",
    "    runnable = prompt_rag | llm.with_structured_output(schema=Answer)\n",
    "    \n",
    "    # Shuffle the answers and get the letter of the correct answer\n",
    "    answers, correct_letter = shuffle_answers(q['wrong_answers'], q['correct_answer'])\n",
    "\n",
    "    # Retrieve relevant chunks and concatenate them\n",
    "    retrieved_docs = retriever.invoke(q['question'])\n",
    "    chunks = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    \n",
    "    # Create the text with question and answers\n",
    "    text = f\"\"\"{q['question']}\n",
    "    A) {answers[0]}\n",
    "    B) {answers[1]}\n",
    "    C) {answers[2]}\n",
    "    D) {answers[3]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Invoke the runnable\n",
    "    response = runnable.invoke(\n",
    "        {\n",
    "        \"text\": text,\n",
    "        \"context\": chunks\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    answer_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "    print(f\"Question: {q['question']}\")\n",
    "    print(f\"Answer: {response.answer} ({answers[answer_mapping[response.answer]]})\")\n",
    "    print(f\"The correct answer: {correct_letter} ({q['correct_answer']})\\n\")\n",
    "    print(f\"The answer was correct: {response.answer == correct_letter}\\n\")\n",
    "    if response.answer == correct_letter:\n",
    "        count_correct += 1\n",
    "\n",
    "print(f\"Correct answers: {count_correct}\\n\"\n",
    "    f\"Total questions: {len(dataset['questions'])}\\n\"\n",
    "    f\"Accuracy: {count_correct/len(dataset['questions'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanved RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_hyde = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a hypothetical document creator.\"\n",
    "            \"Given a question, you generate one to three sentences of narrative text that contains answers for the question.\"\n",
    "        ),\n",
    "        (\"human\", \"\"\"Here is a question:\n",
    "         \n",
    "        {question}\n",
    "         \n",
    "        Generate a narrative text that contains answers to the question.\"\"\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ship had flapped out of a gun-metal sky over Oregon, its shape that of a hideously magnified paramecium with edges that rippled like a mythological flying carpet. Its five green-skinned, froglike occupants had delivered the ultimatum, one copy printed on velvety paper to each major government, each copy couched faultlessly in the appropriate native tongue:\n",
      "A scuffing sound intruded as the five green-skinned figures shuffled forward. They were trembling, and Francine saw glistening drops of wetness below their crests. Their eyes blinked. She sensed the aura of sadness about them, and new tears welled in her eyes.\n",
      "Anger coursed through her. She stopped on the steps, stood there shivering. A new feeling of futility replaced the anger. Tears blurred her vision. What can one lone woman do against such ruthless schemers?\n",
      "The doors of the spaceship opened. Five green-skinned figures emerged. They stopped, stood staring at her, their shoulders slumped. Simultaneously, Francine felt the thickened air relax its hold upon her. She strode forward, tears coursing down her cheeks.\n",
      "\n",
      "\"You made them afraid!\" she shouted. \"What else could they do? The fearful can't think.\"\n",
      "A series of green-shaded lights suspended above this table gave the scene an odd resemblance to a gambling room. The effect was heightened by the set look to the shoulders of the men sitting in spring bottom chairs around the table. There were a scattering of uniforms: Air Force, Army, Marines; plus hard-faced civilians in expensive suits.\n",
      "Anger coursed through her. She stopped on the steps, stood there shivering. A new feeling of futility replaced the anger. Tears blurred her vision. What can one lone woman do against such ruthless schemers?\n",
      "\n",
      "Through her tears, she saw movement on the concourse: a man in civilian clothes crossing from right to left. Her mind registered the movement with only partial awareness: man stops, points. She was suddenly alert, tears gone, following the direction of the civilian's extended right arm, hearing his voice shout: \"Hey! Look at that!\"\n",
      "\n",
      "A thin needle of an aircraft stitched a hurtling line across the watery desert sky. It banked, arrowed toward the spaceship. Behind it roared an airforce jet—delta wings vibrating, sun flashing off polished metal. Tracers laced out toward the airship.\n",
      "\n",
      "Someone's attacking the spaceship! she thought. It's a Russian ICBM!\n",
      "\n",
      "But the needle braked abruptly, impossibly, over the spaceship. Behind it, the airforce jet's engine died, and there was only the eerie whistling of air burning across its wings.\n",
      "\n",
      "Gently, the needle lowered itself into a fold of the spaceship.\n",
      "\n",
      "It's one of theirs—the Galactics' she realized. Why is it coming here now? Do they suspect attack? Is that some kind of reinforcement?\n",
      "\n",
      "Deprived of its power, the jet staggered, skimmed out to a dust-geyser, belly-landing in the alkali flats. Sirens screamed as emergency vehicles raced toward it.\n",
      "Chunk 0: The ship had flapped out of a gun-metal sky over Oregon, its shape that of a hideously magnified paramecium with edges that rippled like a mythological flying carpet. Its five green-skinned, froglike occupants had delivered the ultimatum, one copy printed on velvety paper to each major government, each copy couched faultlessly in the appropriate native tongue:\n",
      "\n",
      "Chunk 0 similarity: 0.6014148679463325\n",
      "\n",
      "Chunk 1: A scuffing sound intruded as the five green-skinned figures shuffled forward. They were trembling, and Francine saw glistening drops of wetness below their crests. Their eyes blinked. She sensed the aura of sadness about them, and new tears welled in her eyes.\n",
      "\n",
      "Chunk 1 similarity: 0.7943260316168177\n",
      "\n",
      "Chunk 2: Anger coursed through her. She stopped on the steps, stood there shivering. A new feeling of futility replaced the anger. Tears blurred her vision. What can one lone woman do against such ruthless schemers?\n",
      "\n",
      "Chunk 2 similarity: 0.6931887842757665\n",
      "\n",
      "Chunk 3: The doors of the spaceship opened. Five green-skinned figures emerged. They stopped, stood staring at her, their shoulders slumped. Simultaneously, Francine felt the thickened air relax its hold upon her. She strode forward, tears coursing down her cheeks.\n",
      "\n",
      "\"You made them afraid!\" she shouted. \"What else could they do? The fearful can't think.\"\n",
      "\n",
      "Chunk 3 similarity: 0.7110015126226195\n",
      "\n",
      "Chunk 4: A series of green-shaded lights suspended above this table gave the scene an odd resemblance to a gambling room. The effect was heightened by the set look to the shoulders of the men sitting in spring bottom chairs around the table. There were a scattering of uniforms: Air Force, Army, Marines; plus hard-faced civilians in expensive suits.\n",
      "\n",
      "Chunk 4 similarity: 0.8247575369548964\n",
      "\n",
      "Chunk 5: Anger coursed through her. She stopped on the steps, stood there shivering. A new feeling of futility replaced the anger. Tears blurred her vision. What can one lone woman do against such ruthless schemers?\n",
      "\n",
      "Through her tears, she saw movement on the concourse: a man in civilian clothes crossing from right to left. Her mind registered the movement with only partial awareness: man stops, points. She was suddenly alert, tears gone, following the direction of the civilian's extended right arm, hearing his voice shout: \"Hey! Look at that!\"\n",
      "\n",
      "A thin needle of an aircraft stitched a hurtling line across the watery desert sky. It banked, arrowed toward the spaceship. Behind it roared an airforce jet—delta wings vibrating, sun flashing off polished metal. Tracers laced out toward the airship.\n",
      "\n",
      "Someone's attacking the spaceship! she thought. It's a Russian ICBM!\n",
      "\n",
      "But the needle braked abruptly, impossibly, over the spaceship. Behind it, the airforce jet's engine died, and there was only the eerie whistling of air burning across its wings.\n",
      "\n",
      "Gently, the needle lowered itself into a fold of the spaceship.\n",
      "\n",
      "It's one of theirs—the Galactics' she realized. Why is it coming here now? Do they suspect attack? Is that some kind of reinforcement?\n",
      "\n",
      "Deprived of its power, the jet staggered, skimmed out to a dust-geyser, belly-landing in the alkali flats. Sirens screamed as emergency vehicles raced toward it.\n",
      "\n",
      "Chunk 5 similarity: 0.623101953787299\n",
      "\n",
      "Question: What threat did the newspaper headlines blare?\n",
      "Answer: A (Talk or Die!)\n",
      "The correct answer: A (Talk or Die!)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Chunk 0: Some five miles east of the site the dust storm hazed across the monotonous structures of the cantonment that housed some thirty thousand people from every major nation: Linguists, anthropologists, psychologists, doctors of every shape and description, watchers and watchers for the watchers, spies, espionage and counter-espionage agents.\n",
      "\n",
      "For seven months the threat of Eniwetok, the threat of the unknown as well, had held them in check.\n",
      "\n",
      "Toward evening of this day the wind slackened. The drifted sand began sifting off the ship and back into new shapes, trickling down for all the world like the figurative \"sands of time\" that here were most certainly running out.\n",
      "\n",
      "Mrs. Francine Millar, clinical psychologist with the Indo-European Germanic-Root team, hurried across the bare patch of trampled sand outside the spaceship's entrance. She bent her head against what was left of the windstorm. Under her left arm she carried her briefcase tucked up like a football. Her other hand carried a rolled-up copy of that afternoon's Oregon Journal. The lead story said that Air Force jets had shot down a small private plane trying to sneak into the restricted area. Three unidentified men killed. The plane had been stolen.\n",
      "\n",
      "Chunk 0 similarity: 0.7606236981897369\n",
      "\n",
      "Chunk 1: For seven months the threat of Eniwetok, the threat of the unknown as well, had held them in check.\n",
      "\n",
      "Toward evening of this day the wind slackened. The drifted sand began sifting off the ship and back into new shapes, trickling down for all the world like the figurative \"sands of time\" that here were most certainly running out.\n",
      "\n",
      "Mrs. Francine Millar, clinical psychologist with the Indo-European Germanic-Root team, hurried across the bare patch of trampled sand outside the spaceship's entrance. She bent her head against what was left of the windstorm. Under her left arm she carried her briefcase tucked up like a football. Her other hand carried a rolled-up copy of that afternoon's Oregon Journal. The lead story said that Air Force jets had shot down a small private plane trying to sneak into the restricted area. Three unidentified men killed. The plane had been stolen.\n",
      "\n",
      "Chunk 1 similarity: 0.7726766111242779\n",
      "\n",
      "Chunk 2: The ship had flapped out of a gun-metal sky over Oregon, its shape that of a hideously magnified paramecium with edges that rippled like a mythological flying carpet. Its five green-skinned, froglike occupants had delivered the ultimatum, one copy printed on velvety paper to each major government, each copy couched faultlessly in the appropriate native tongue:\n",
      "\n",
      "Chunk 2 similarity: 0.8415737424716654\n",
      "\n",
      "Chunk 3: For seven months the threat of Eniwetok, the threat of the unknown as well, had held them in check.\n",
      "\n",
      "Toward evening of this day the wind slackened. The drifted sand began sifting off the ship and back into new shapes, trickling down for all the world like the figurative \"sands of time\" that here were most certainly running out.\n",
      "\n",
      "Chunk 3 similarity: 0.8730898675061273\n",
      "\n",
      "Chunk 4: All day long a damp wind poured up the Columbia Gorge from the ocean. It swept across the Eastern Oregon alkali flats with a false prediction of rain. Spiny desert scrub bent before the gusts, sheltering blur-footed coveys of quail and flop-eared jackrabbits. Heaps of tumbleweed tangled in the fence lines, and the air was filled with dry particles of grit that crept under everything and into everything and onto everything with the omnipresence of filterable virus.\n",
      "\n",
      "Chunk 4 similarity: 0.9400871141373321\n",
      "\n",
      "Chunk 5: Every mind on earth capable of understanding the problem was focused on the spaceship and the ultimatum delivered by its occupants. Talk or Die! blared the newspaper headlines.\n",
      "\n",
      "The suicide rate was up and still climbing. Religious cults were having a field day. A book by a science fiction author: \"What the Deadly Inter-Galactic Spaceship Means to You!\" had smashed all previous best-seller records. And this had been going on for a frantic seven months.\n",
      "\n",
      "Chunk 5 similarity: 0.8036268276166584\n",
      "\n",
      "Question: What is the name of the protagonist?\n",
      "Answer: A (Francine Millar)\n",
      "The correct answer: A (Francine Millar)\n",
      "\n",
      "The answer was correct: True\n",
      "\n",
      "Correct answers: 2\n",
      "Total questions: 38\n",
      "Accuracy: 0.05263157894736842\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct_answers = 0\n",
    "\n",
    "retrieved_docs = retriever.invoke(\"\")\n",
    "sample_text = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "print(sample_text)\n",
    "\n",
    "for q in dataset['questions']:\n",
    "    runnable = prompt_rag | llm.with_structured_output(schema=Answer)\n",
    "    hyde = prompt_hyde | llm\n",
    "    \n",
    "    # Shuffle the answers and get the letter of the correct answer\n",
    "    answers, correct_letter = shuffle_answers(q['wrong_answers'], q['correct_answer'])\n",
    "\n",
    "    hyde_response = hyde.invoke(\n",
    "        {\n",
    "        \"question\": q['question']\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    embedding_question = client.embeddings.create(\n",
    "        input=q['question'],\n",
    "        model=\"text-embedding-3-small\"\n",
    "    ).data[0].embedding\n",
    "\n",
    "    for i, doc in enumerate(retrieved_docs):\n",
    "        embedding_chunk = client.embeddings.create(\n",
    "            input=doc.page_content,\n",
    "            model=\"text-embedding-3-small\"\n",
    "        ).data[0].embedding\n",
    "        print(f\"Chunk {i}: {doc.page_content}\\n\")\n",
    "        print(f\"Chunk {i} similarity: {cosine(embedding_question, embedding_chunk)}\\n\")\n",
    "\n",
    "    # Retrieve relevant chunks and concatenate them\n",
    "    retrieved_docs = retriever.invoke(hyde_response.content)\n",
    "    chunks = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "    # Create the text with question and answers\n",
    "    text = f\"\"\"{q['question']}\n",
    "    A) {answers[0]}\n",
    "    B) {answers[1]}\n",
    "    C) {answers[2]}\n",
    "    D) {answers[3]}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Invoke the runnable\n",
    "    response = runnable.invoke(\n",
    "        {\n",
    "        \"text\": text,\n",
    "        \"context\": chunks\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Print the results\n",
    "    answer_mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3}\n",
    "    print(f\"Question: {q['question']}\")\n",
    "    print(f\"Answer: {response.answer} ({answers[answer_mapping[response.answer]]})\")\n",
    "    print(f\"The correct answer: {correct_letter} ({q['correct_answer']})\\n\")\n",
    "    print(f\"The answer was correct: {response.answer == correct_letter}\\n\")\n",
    "    if response.answer == correct_letter:\n",
    "        correct_answers += 1\n",
    "print(f\"Correct answers: {correct_answers}\\n\"\n",
    "    f\"Total questions: {len(dataset['questions'])}\\n\"\n",
    "    f\"Accuracy: {correct_answers/len(dataset['questions'])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One pipeline for all four systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for the LLM baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_llm_baseline(runnable, question_prompt):\n",
    "    \n",
    "    # Start a timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Invoke the runnable\n",
    "    response = runnable.invoke(\n",
    "        {\n",
    "            \"text\": question_prompt\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Calculate the processing time\n",
    "    processing_time = time.time() - start_time\n",
    "\n",
    "    return response.answer, processing_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for the LLM with full book as context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_llm_full_context(runnable, question_prompt):\n",
    "    \n",
    "    # Start a timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Invoke the runnable, with the full book as context\n",
    "    response = runnable.invoke(\n",
    "        {\n",
    "        \"text\": question_prompt,\n",
    "        \"context\": full_book\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Calculate the processing time\n",
    "    processing_time = time.time() - start_time\n",
    "\n",
    "    return response.answer, processing_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for the Naive RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_naive_rag(runnable, question_prompt, question):\n",
    "\n",
    "    # Start a timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Retrieve most relevant document chunks and concatenate them\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    chunks = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    \n",
    "    # Invoke the runnable with the question and chunks\n",
    "    response = runnable.invoke(\n",
    "        {\n",
    "        \"text\": question_prompt,\n",
    "        \"context\": chunks\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Calculate the processing time\n",
    "    processing_time = time.time() - start_time\n",
    "\n",
    "    return response.answer, processing_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for the Advanced RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_advanced_rag(runnable, runnable_hyde, question_prompt, question):\n",
    "\n",
    "    # Start a timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate a hypothetical document based on the question\n",
    "    hyde_response = runnable_hyde.invoke(\n",
    "        {\n",
    "        \"question\": question,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Retrieve relevant chunks based on the hypothetical document\n",
    "    retrieved_docs = retriever.invoke(hyde_response.content)\n",
    "    chunks = \"\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    \n",
    "    # Invoke the runnable, with the question and chunks\n",
    "    response = runnable.invoke(\n",
    "        {\n",
    "        \"text\": question_prompt,\n",
    "        \"context\": chunks\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Calculate the processing time\n",
    "    processing_time = time.time() - start_time\n",
    "\n",
    "    return response.answer, processing_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can apply the pipeline to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LLM runnable without context\n",
    "runnable_no_context = prompt_baseline_llm | llm.with_structured_output(schema=Answer)\n",
    "\n",
    "# Create the runnable for RAG\n",
    "runnable_rag = prompt_rag | llm.with_structured_output(schema=Answer)\n",
    "\n",
    "# Create the runnable for HyDE\n",
    "runnable_hyde = prompt_hyde | llm\n",
    "\n",
    "results = {\n",
    "    \"correct_answers\": [],\n",
    "    \"llm_baseline\": [],\n",
    "    \"llm_full_context\": [],\n",
    "    \"naive_rag\": [],\n",
    "    \"advanced_rag\": []\n",
    "}\n",
    "\n",
    "total_processing_time_llm_baseline = []\n",
    "total_processing_time_llm_full_context = []\n",
    "total_processing_time_naive_rag = []\n",
    "total_processing_time_advanced_rag = []\n",
    "\n",
    "for q in dataset['questions']:\n",
    "\n",
    "    # Shuffle the answers and store the correct answer\n",
    "    answers, correct_letter = shuffle_answers(q['wrong_answers'], q['correct_answer'])\n",
    "    results[\"correct_answers\"].append(correct_letter)\n",
    "\n",
    "    # Create the text with question and answers\n",
    "    question_prompt = f\"\"\"{q['question']}\n",
    "\n",
    "    A) {answers[0]}\n",
    "    B) {answers[1]}\n",
    "    C) {answers[2]}\n",
    "    D) {answers[3]}\n",
    "    \\nAnswer with only the letter of the correct answer. If you are unsure, take your best guess.\n",
    "    \"\"\"\n",
    "\n",
    "    # Run the LLM baseline\n",
    "    llm_baseline_answer, llm_baseline_processing_time = answer_llm_baseline(runnable_no_context, question_prompt)\n",
    "    results['llm_baseline'].append(llm_baseline_answer)\n",
    "    total_processing_time_llm_baseline.append(llm_baseline_processing_time)\n",
    "\n",
    "    # Run the LLM with full context\n",
    "    llm_full_context_answer, llm_full_context_processing_time = answer_llm_full_context(runnable_rag, question_prompt)\n",
    "    results['llm_full_context'].append(llm_full_context_answer)\n",
    "    total_processing_time_llm_full_context.append(llm_full_context_processing_time)\n",
    "\n",
    "    # Run the naive RAG\n",
    "    naive_rag_answer, naive_rag_processing_time = answer_naive_rag(runnable_rag, question_prompt, q['question'])\n",
    "    results['naive_rag'].append(naive_rag_answer)\n",
    "    total_processing_time_naive_rag.append(naive_rag_processing_time)\n",
    "\n",
    "    # Run the advanced RAG\n",
    "    advanced_rag_answer, advanced_rag_processing_time = answer_advanced_rag(runnable_rag, runnable_hyde, question_prompt, q['question'])\n",
    "    results['advanced_rag'].append(advanced_rag_answer)\n",
    "    total_processing_time_advanced_rag.append(advanced_rag_processing_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM baseline:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.14      0.14      0.14         7\n",
      "           B       0.27      0.33      0.30         9\n",
      "           C       0.64      0.64      0.64        14\n",
      "           D       0.38      0.30      0.33        10\n",
      "\n",
      "    accuracy                           0.40        40\n",
      "   macro avg       0.36      0.35      0.35        40\n",
      "weighted avg       0.41      0.40      0.40        40\n",
      "\n",
      "LLM full context:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.71      0.71      0.71         7\n",
      "           B       0.89      0.89      0.89         9\n",
      "           C       0.81      0.93      0.87        14\n",
      "           D       1.00      0.80      0.89        10\n",
      "\n",
      "    accuracy                           0.85        40\n",
      "   macro avg       0.85      0.83      0.84        40\n",
      "weighted avg       0.86      0.85      0.85        40\n",
      "\n",
      "Naive RAG:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.67      0.57      0.62         7\n",
      "           B       0.70      0.78      0.74         9\n",
      "           C       0.83      0.71      0.77        14\n",
      "           D       0.67      0.80      0.73        10\n",
      "\n",
      "    accuracy                           0.73        40\n",
      "   macro avg       0.72      0.72      0.71        40\n",
      "weighted avg       0.73      0.72      0.72        40\n",
      "\n",
      "Advanced RAG:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.50      0.43      0.46         7\n",
      "           B       0.70      0.78      0.74         9\n",
      "           C       0.83      0.71      0.77        14\n",
      "           D       0.75      0.90      0.82        10\n",
      "\n",
      "    accuracy                           0.73        40\n",
      "   macro avg       0.70      0.71      0.70        40\n",
      "weighted avg       0.72      0.72      0.72        40\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the classification report for each method\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"LLM baseline:\")\n",
    "print(classification_report(results['correct_answers'], results['llm_baseline']))\n",
    "\n",
    "print(\"LLM full context:\")\n",
    "print(classification_report(results['correct_answers'], results['llm_full_context']))\n",
    "\n",
    "print(\"Naive RAG:\")\n",
    "print(classification_report(results['correct_answers'], results['naive_rag']))\n",
    "\n",
    "print(\"Advanced RAG:\")\n",
    "print(classification_report(results['correct_answers'], results['advanced_rag']))\n",
    "\n",
    "# Store the raw results, classification reports and processing times to a JSON file\n",
    "results_dict = {\n",
    "    \"results\": results,\n",
    "    \"classification_reports\": {\n",
    "        \"llm_baseline\": classification_report(results['correct_answers'], results['llm_baseline'], output_dict=True),\n",
    "        \"llm_full_context\": classification_report(results['correct_answers'], results['llm_full_context'], output_dict=True),\n",
    "        \"naive_rag\": classification_report(results['correct_answers'], results['naive_rag'], output_dict=True),\n",
    "        \"advanced_rag\": classification_report(results['correct_answers'], results['advanced_rag'], output_dict=True)\n",
    "    },\n",
    "    \"processing_times\": {\n",
    "        \"llm_baseline\": total_processing_time_llm_baseline,\n",
    "        \"llm_full_context\": total_processing_time_llm_full_context,\n",
    "        \"naive_rag\": total_processing_time_naive_rag,\n",
    "        \"advanced_rag\": total_processing_time_advanced_rag\n",
    "    }\n",
    "}\n",
    "\n",
    "current_time = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "filename = f\"results-{current_time}.json\"\n",
    "\n",
    "with open(filename, \"w\") as file:\n",
    "    json.dump(results_dict, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average processing time LLM baseline: 0.623898035287857\n",
      "Minimum processing time LLM baseline: 0.4306309223175049\n",
      "Maximum processing time LLM baseline: 1.773648977279663\n",
      "Average processing time LLM full context: 4.025880527496338\n",
      "Minimum processing time LLM full context: 1.5085179805755615\n",
      "Maximum processing time LLM full context: 74.44881200790405\n",
      "Average processing time Naive RAG: 1.0835974156856536\n",
      "Minimum processing time Naive RAG: 0.6783907413482666\n",
      "Maximum processing time Naive RAG: 2.6651787757873535\n",
      "Average processing time Advanced RAG: 2.6325348913669586\n",
      "Minimum processing time Advanced RAG: 1.8137741088867188\n",
      "Maximum processing time Advanced RAG: 5.87543511390686\n"
     ]
    }
   ],
   "source": [
    "# Print the average, minimum and maximum processing time for each method\n",
    "print(f\"Average processing time LLM baseline: {sum(total_processing_time_llm_baseline)/len(total_processing_time_llm_baseline)}\")\n",
    "print(f\"Minimum processing time LLM baseline: {min(total_processing_time_llm_baseline)}\")\n",
    "print(f\"Maximum processing time LLM baseline: {max(total_processing_time_llm_baseline)}\")\n",
    "\n",
    "print(f\"Average processing time LLM full context: {sum(total_processing_time_llm_full_context)/len(total_processing_time_llm_full_context)}\")\n",
    "print(f\"Minimum processing time LLM full context: {min(total_processing_time_llm_full_context)}\")\n",
    "print(f\"Maximum processing time LLM full context: {max(total_processing_time_llm_full_context)}\")\n",
    "\n",
    "print(f\"Average processing time Naive RAG: {sum(total_processing_time_naive_rag)/len(total_processing_time_naive_rag)}\")\n",
    "print(f\"Minimum processing time Naive RAG: {min(total_processing_time_naive_rag)}\")\n",
    "print(f\"Maximum processing time Naive RAG: {max(total_processing_time_naive_rag)}\")\n",
    "\n",
    "print(f\"Average processing time Advanced RAG: {sum(total_processing_time_advanced_rag)/len(total_processing_time_advanced_rag)}\")\n",
    "print(f\"Minimum processing time Advanced RAG: {min(total_processing_time_advanced_rag)}\")\n",
    "print(f\"Maximum processing time Advanced RAG: {max(total_processing_time_advanced_rag)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate performance per question type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIRECT RETRIEVAL\n",
      "Baseline LLM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00         2\n",
      "           B       0.00      0.00      0.00         3\n",
      "           C       0.00      0.00      0.00         3\n",
      "           D       0.33      0.50      0.40         2\n",
      "\n",
      "    accuracy                           0.10        10\n",
      "   macro avg       0.08      0.12      0.10        10\n",
      "weighted avg       0.07      0.10      0.08        10\n",
      "\n",
      "Full context LLM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00         2\n",
      "           B       1.00      1.00      1.00         3\n",
      "           C       1.00      1.00      1.00         3\n",
      "           D       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           1.00        10\n",
      "   macro avg       1.00      1.00      1.00        10\n",
      "weighted avg       1.00      1.00      1.00        10\n",
      "\n",
      "Naive RAG:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00         2\n",
      "           B       1.00      0.67      0.80         3\n",
      "           C       0.75      1.00      0.86         3\n",
      "           D       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.94      0.92      0.91        10\n",
      "weighted avg       0.93      0.90      0.90        10\n",
      "\n",
      "Advanced RAG:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.50      0.50      0.50         2\n",
      "           B       1.00      0.67      0.80         3\n",
      "           C       0.67      0.67      0.67         3\n",
      "           D       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.71      0.71      0.69        10\n",
      "weighted avg       0.73      0.70      0.70        10\n",
      "\n",
      "PARAPHRASED RETRIEVAL\n",
      "Baseline LLM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.50      0.33      0.40         3\n",
      "           B       0.00      0.00      0.00         2\n",
      "           C       0.25      1.00      0.40         1\n",
      "           D       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.20        10\n",
      "   macro avg       0.19      0.33      0.20        10\n",
      "weighted avg       0.17      0.20      0.16        10\n",
      "\n",
      "Full context LLM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      1.00      1.00         3\n",
      "           B       0.67      1.00      0.80         2\n",
      "           C       1.00      1.00      1.00         1\n",
      "           D       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.90        10\n",
      "   macro avg       0.92      0.94      0.91        10\n",
      "weighted avg       0.93      0.90      0.90        10\n",
      "\n",
      "Naive RAG:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.67      0.80         3\n",
      "           B       0.67      1.00      0.80         2\n",
      "           C       0.50      1.00      0.67         1\n",
      "           D       0.67      0.50      0.57         4\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.71      0.79      0.71        10\n",
      "weighted avg       0.75      0.70      0.70        10\n",
      "\n",
      "Advanced RAG:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       1.00      0.67      0.80         3\n",
      "           B       0.50      1.00      0.67         2\n",
      "           C       1.00      1.00      1.00         1\n",
      "           D       1.00      0.75      0.86         4\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.88      0.85      0.83        10\n",
      "weighted avg       0.90      0.80      0.82        10\n",
      "\n",
      "INFERENCE\n",
      "Baseline LLM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00         1\n",
      "           B       0.00      0.00      0.00         0\n",
      "           C       0.86      0.86      0.86         7\n",
      "           D       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.46      0.34      0.38        10\n",
      "weighted avg       0.80      0.70      0.73        10\n",
      "\n",
      "Full context LLM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00         1\n",
      "           C       0.78      1.00      0.88         7\n",
      "           D       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.59      0.50      0.51        10\n",
      "weighted avg       0.74      0.80      0.75        10\n",
      "\n",
      "Naive RAG:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00         1\n",
      "           B       0.00      0.00      0.00         0\n",
      "           C       1.00      0.71      0.83         7\n",
      "           D       0.50      1.00      0.67         2\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.38      0.43      0.38        10\n",
      "weighted avg       0.80      0.70      0.72        10\n",
      "\n",
      "Advanced RAG:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00         1\n",
      "           B       0.00      0.00      0.00         0\n",
      "           C       1.00      0.86      0.92         7\n",
      "           D       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.80        10\n",
      "   macro avg       0.50      0.46      0.48        10\n",
      "weighted avg       0.90      0.80      0.85        10\n",
      "\n",
      "THEMATIC INSIGHT\n",
      "Baseline LLM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00         1\n",
      "           B       1.00      0.75      0.86         4\n",
      "           C       1.00      0.67      0.80         3\n",
      "           D       0.50      0.50      0.50         2\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.62      0.48      0.54        10\n",
      "weighted avg       0.80      0.60      0.68        10\n",
      "\n",
      "Full context LLM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00         1\n",
      "           B       1.00      0.75      0.86         4\n",
      "           C       0.67      0.67      0.67         3\n",
      "           D       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.67      0.60      0.63        10\n",
      "weighted avg       0.80      0.70      0.74        10\n",
      "\n",
      "Naive RAG:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00         1\n",
      "           B       0.75      0.75      0.75         4\n",
      "           C       1.00      0.33      0.50         3\n",
      "           D       0.67      1.00      0.80         2\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.60      0.52      0.51        10\n",
      "weighted avg       0.73      0.60      0.61        10\n",
      "\n",
      "Advanced RAG:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00         1\n",
      "           B       1.00      0.75      0.86         4\n",
      "           C       0.50      0.33      0.40         3\n",
      "           D       0.50      1.00      0.67         2\n",
      "\n",
      "    accuracy                           0.60        10\n",
      "   macro avg       0.50      0.52      0.48        10\n",
      "weighted avg       0.65      0.60      0.60        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/kris/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Split the results into 4 groups, the first 10, the second, the thurd and fourth\n",
    "direct_retrieval_results = {key: value[:10] for key, value in results.items()}\n",
    "paraphrased_retrieval_results = {key: value[10:20] for key, value in results.items()}\n",
    "inference_results = {key: value[20:30] for key, value in results.items()}\n",
    "thematic_insight_results = {key: value[30:] for key, value in results.items()}\n",
    "\n",
    "# Calculate the classification report for each method\n",
    "\n",
    "print(\"DIRECT RETRIEVAL\")\n",
    "print(\"Baseline LLM:\")\n",
    "print(classification_report(direct_retrieval_results['correct_answers'], direct_retrieval_results['llm_baseline']))\n",
    "print(\"Full context LLM:\")\n",
    "print(classification_report(direct_retrieval_results['correct_answers'], direct_retrieval_results['llm_full_context']))\n",
    "print(\"Naive RAG:\")\n",
    "print(classification_report(direct_retrieval_results['correct_answers'], direct_retrieval_results['naive_rag']))\n",
    "print(\"Advanced RAG:\")\n",
    "print(classification_report(direct_retrieval_results['correct_answers'], direct_retrieval_results['advanced_rag']))\n",
    "\n",
    "print(\"PARAPHRASED RETRIEVAL\")\n",
    "print(\"Baseline LLM:\")\n",
    "print(classification_report(paraphrased_retrieval_results['correct_answers'], paraphrased_retrieval_results['llm_baseline']))\n",
    "print(\"Full context LLM:\")\n",
    "print(classification_report(paraphrased_retrieval_results['correct_answers'], paraphrased_retrieval_results['llm_full_context']))\n",
    "print(\"Naive RAG:\")\n",
    "print(classification_report(paraphrased_retrieval_results['correct_answers'], paraphrased_retrieval_results['naive_rag']))\n",
    "print(\"Advanced RAG:\")\n",
    "print(classification_report(paraphrased_retrieval_results['correct_answers'], paraphrased_retrieval_results['advanced_rag']))\n",
    "\n",
    "print(\"INFERENCE\")\n",
    "print(\"Baseline LLM:\")\n",
    "print(classification_report(inference_results['correct_answers'], inference_results['llm_baseline']))\n",
    "print(\"Full context LLM:\")\n",
    "print(classification_report(inference_results['correct_answers'], inference_results['llm_full_context']))\n",
    "print(\"Naive RAG:\")\n",
    "print(classification_report(inference_results['correct_answers'], inference_results['naive_rag']))\n",
    "print(\"Advanced RAG:\")\n",
    "print(classification_report(inference_results['correct_answers'], inference_results['advanced_rag']))\n",
    "\n",
    "print(\"THEMATIC INSIGHT\")\n",
    "print(\"Baseline LLM:\")\n",
    "print(classification_report(thematic_insight_results['correct_answers'], thematic_insight_results['llm_baseline']))\n",
    "print(\"Full context LLM:\")\n",
    "print(classification_report(thematic_insight_results['correct_answers'], thematic_insight_results['llm_full_context']))\n",
    "print(\"Naive RAG:\")\n",
    "print(classification_report(thematic_insight_results['correct_answers'], thematic_insight_results['naive_rag']))\n",
    "print(\"Advanced RAG:\")\n",
    "print(classification_report(thematic_insight_results['correct_answers'], thematic_insight_results['advanced_rag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
